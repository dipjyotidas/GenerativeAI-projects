{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e8c809-1173-488c-95fb-eec2caf745dc",
   "metadata": {},
   "source": [
    "### Document summarization application with Open Source Llama2 LLM with Langchain using Sagemaker Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5109f-902b-4980-983c-0cc55ee1be4c",
   "metadata": {},
   "source": [
    "* Author : Dipjyoti Das\n",
    "* Last Edited : Jan 26, 2024\n",
    "* This notebook provides an example for how to use Sagemaker Jumpstart -for text summarization use case on a Fannie Mae public pdf document. It uses Llama-7b-chat fine tuned open source model from Jumsptart model hub with Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368acb2d-fd10-4689-9a89-e379ef114da9",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "* AWS Innovation Sandbox should be installed and Domain created in Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f709c05-370c-4f18-ade3-74d099c29bd1",
   "metadata": {},
   "source": [
    "* Before deploying model endpoint on an instance, you have to request Service Quota limit increase for that particular instance. Check the form link on the confluence page. It might take upto a day for the request to be approved for the associated Sagemaker account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01b647-3c81-4199-8fe4-4386d2715bd3",
   "metadata": {},
   "source": [
    "* Upload the example pdf document ('6_extracted_FM-esg-report-2022.pdf') from Conflunce on your Sagemaker Jupyterlab folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed01b1f-d9cc-4417-bb93-e0611ceb8993",
   "metadata": {},
   "source": [
    "#### Minimum Instance sizes for the following Llama2 Foundational models in Jumpstart:\n",
    "* Llama2-7b-chat : ml.g5.2xlarge\n",
    "* Llama2-13b-chat : ml.g5.12xlarge\n",
    "* Llama2-70b-chat : ml.g5.48xlarge or ml.p4d.24xlarge\n",
    "* Pls request service quota increase for instance associated with the Sagemaker account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b793035-8e11-42fd-8773-c04fede5623e",
   "metadata": {},
   "source": [
    "#### Endpoint Names of the deployed Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a364788d-484f-42c5-a7b4-720192907fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Endpoint name and InferenceComponentName after you deploy the llama2_7b_chat model from Jumpstart model hub on an instance (ml.g5.2xlarge). \n",
    "# Navigate to Endpoint summary -> Test Inference -> Testing Options ->  Use Python SDK example code -> Example inference request\n",
    "\n",
    "llama2_7b_chat_endpoint_name = 'jumpstart-dft-meta-textgeneration-l-20240126-180653'\n",
    "llama2_7b_chat_InferenceComponentName = 'meta-textgeneration-llama-2-7b-f-20240126-180653'\n",
    "\n",
    "# Get the region name from the Sagemaker account\n",
    "region_name = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2241a07-4799-48f3-b029-11a7e6e58fef",
   "metadata": {},
   "source": [
    "#### Install and import the required Libraries with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7ad2ca7-8254-4ba9-bb6f-3897a71cc89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.27\n"
     ]
    }
   ],
   "source": [
    "# Import the Boto3 and JSON modules\n",
    "import json\n",
    "import boto3\n",
    "print(boto3.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8437f315-a37e-42b8-97c2-831d5f1db87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers pypdfium2 accelerate langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27062064-1102-45f5-a3fd-6b36990655c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant modules and break down the long document into chunks:\n",
    "import langchain\n",
    "from langchain import SagemakerEndpoint, PromptTemplate\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain import LLMChain\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFium2Loader\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee9c96f8-b915-46d0-bfdb-c4a86e5f58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filepath of the Fannie Mae public docucment uploaded in the sagemaker directory:\n",
    "pdf_filepath = '/home/sagemaker-user/6_extracted_FM-esg-report-2022.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6b8b935-e092-4983-a5c7-48e8df83900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a pdf document and split in into chunks using Character Text Splitter function from Langchain\n",
    "def pdf_output(pdf_filepath):\n",
    "    loader = PyPDFium2Loader(pdf_filepath)\n",
    "    data = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts_FM1 = text_splitter.split_documents(data)\n",
    "    return texts_FM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f8c65a4-0bb2-40a0-9b4a-e1759f779764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='About Fannie Mae\\r\\nWho we are\\r\\nThe Federal National Mortgage Association, better known as \\r\\nFannie Mae, is a purpose-driven company by charter and by \\r\\nchoice. Our business supports mortgage lenders by providing \\r\\nmortgage financing to help people buy or rent a home. We help \\r\\nmake the popular 30-year fixed-rate mortgage possible, enabling \\r\\npredictable mortgage payments over the life of the loan and \\r\\ngiving homeowners stability and peace of mind. \\r\\nOur charter, an act of Congress, establishes our purposes: \\r\\nto provide liquidity and stability to the residential mortgage \\r\\nmarket and to promote access to mortgage credit. This mandate \\r\\nincludes facilitating mortgages on housing for low- and \\r\\nmoderate-income families involving a reasonable economic \\r\\nreturn that may be less than the return earned on other \\r\\nactivities. Congress declared that our operations should be \\r\\nfinanced by private capital to the maximum extent feasible. With \\r\\nthese Congressional intentions in mind, we have, principally \\r\\nusing private capital, provided liquidity in the secondary market \\r\\nand expanded housing opportunities throughout the U.S. Fannie \\r\\nMae is committed to maintaining safety and soundness as we \\r\\nwork to fulfill this mission.\\r\\nWe do not originate mortgage loans or lend money directly to \\r\\nborrowers. Rather, we work primarily with lenders who originate \\r\\nloans to borrowers. We acquire and securitize those loans into \\r\\nmortgage-backed securities that we guarantee (which we refer \\r\\nto as Fannie Mae MBS or our MBS). Our revenues are primarily \\r\\ndriven by guaranty fees we receive for assuming the credit risk on \\r\\nloans underlying our MBS. As of December 31, 2022, we owned \\r\\nor guaranteed mortgage assets representing an estimated \\r\\n27% of single-family mortgage debt outstanding and 21% of \\r\\nmultifamily mortgage debt outstanding in the U.S.7\\r\\n7 Based on internal estimates using data from the Federal Reserve Board of Governors’ “Financial Accounts of the United States” (Z.1) release, Q4 2022.\\r\\n© 2023 Fannie Mae 2022 ESG Report | 5\\r\\nTABLE OF CONTENTS INTRODUCTION / ABOUT FANNIE MAE', metadata={'source': '/home/sagemaker-user/6_extracted_FM-esg-report-2022.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the contents of the pdf document:\n",
    "pdf_output(pdf_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110afed-cd3b-4fc0-b06d-a983110eb779",
   "metadata": {},
   "source": [
    "#### Using Langchain for Text Summarization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa41be8-ff3d-4534-8aa6-970990b661ab",
   "metadata": {},
   "source": [
    "* Check :  https://python.langchain.com/docs/integrations/llms/sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dfc12e9f-79a7-45d3-b2f8-4c1e0860c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make LangChain work effectively with Llama models, we need to define the default Content Handler classes for valid input and output:\n",
    "\n",
    "class ContentHandlerTextSummarization(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> json:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        generated_text = response_json[0]['generated_text']\n",
    "        return generated_text.split(\"summary:\")[-1]\n",
    "    \n",
    "content_handler = ContentHandlerTextSummarization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ffd55-16b9-466f-a7d3-5acd47846afa",
   "metadata": {},
   "source": [
    "* Define the model with parameters using Sagemaker Endpoint class from Langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03a02624-bd44-42ce-996b-33d0bb282260",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model_llm = SagemakerEndpoint( endpoint_name=llama2_7b_chat_endpoint_name, \n",
    "                                      region_name= region_name,\n",
    "                                      model_kwargs={\"max_new_tokens\": 2000, \"top_p\": 0.9, \"temperature\": 0.6, \"top_k\":10, \"do_sample\" :True, \"max_length\": 1000},\n",
    "                                      endpoint_kwargs={ \"CustomAttributes\": 'accept_eula=true', \"InferenceComponentName\" : llama2_7b_chat_InferenceComponentName}, \n",
    "                                      content_handler=content_handler )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0469e15e-f635-4299-813d-081b9f5f129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] template='\\n              Write a summary of the following text delimited by triple backquotes.\\n              Return your response in bullet points which covers the key points of the text.\\n              ```{text}```\\n              BULLET POINT SUMMARY:\\n           '\n"
     ]
    }
   ],
   "source": [
    "# Write the custom Prompt Template:\n",
    "template = \"\"\"\n",
    "              Write a summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "           \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91ce6435-2fd8-4443-8077-0b2e4691221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the LLMChain from Langchain library:\n",
    "llm_chain = LLMChain(prompt=prompt, llm=summary_model_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0884b5a-7359-4063-84e3-ff0cb971b2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • Fannie Mae is a purpose-driven company by charter and by choice,\n"
     ]
    }
   ],
   "source": [
    "# Result of Llama-2-7b-chat model\n",
    "print(llm_chain.run(summarize_pdf(pdf_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84872e-ab1c-45c7-9dfb-04ceefac4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summary may not be exactly what you are looking, you have to test it with Llama2-7b-chat model parameters or test with\n",
    "# bigger Llama2-13b or Llama2-70b parameter models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171bbada-1d1a-4b09-857f-6316ffa3e8a6",
   "metadata": {},
   "source": [
    "#### * Putting it all together in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "643ba51c-66b2-4841-9e8b-a90d7a15db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_filepath):\n",
    "    loader = PyPDFium2Loader(pdf_filepath)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    text_FM = text_splitter.split_documents(data)\n",
    "\n",
    "    template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "           \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=summary_model_llm)\n",
    "\n",
    "    return llm_chain.run(text_FM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6c3fd946-d003-4d19-b84d-815be0f2f755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' • Fannie Mae is a purpose-driven company by charter and by choice.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result of Llama2-7b-chat model\n",
    "summarize_pdf(pdf_filepath = pdf_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa463c3-5a48-42af-8542-e803dc3c6ac1",
   "metadata": {},
   "source": [
    "#### Using Gradio UI Interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59c3141d-ffe9-4be5-b7ed-5ae14cdaf686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "90d178c6-9b63-4455-826d-5ec7c1575f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.15.0\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "print(gr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a357d2a2-79b6-4f75-91e4-96a9d62e9ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/6_extracted_FM-esg-report-2022.pdf\n"
     ]
    }
   ],
   "source": [
    "# Get the filepath of the uploaded pdf document in Sagemkaer jupyterlab workspace:\n",
    "print(pdf_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539657e-82a8-448b-9b8a-bef42b677131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the above pdf_filepath in the Gradio: 'Provide PDF file path to get the summary' textbox field and you will see the summarized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa924f08-04d3-4bca-8330-c4401bc828b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "Running on public URL: https://fff323c07ca7ee6033.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://fff323c07ca7ee6033.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_pdf_path = gr.components.Textbox(label=\"Provide the PDF file path\")\n",
    "output_summary = gr.components.Textbox(label=\"Summary\")\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=summarize_pdf_full,\n",
    "    inputs=input_pdf_path,\n",
    "    outputs=output_summary,\n",
    "    title=\"PDF Summarizer\",\n",
    "    description=\"Provide PDF file path to get the summary.\"\n",
    ").queue().launch(share=True, debug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a5374-d1d5-45f8-992f-6c584658fb53",
   "metadata": {},
   "source": [
    "### Don't Forget : Delete the endpoints from the Llama2-7b-chat model notebook, also confirm deletion from the Jumpstart Deployment UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
