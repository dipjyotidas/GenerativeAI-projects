{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e8c809-1173-488c-95fb-eec2caf745dc",
   "metadata": {},
   "source": [
    "### Document summarization application with Open Source Llama2 LLM with Langchain using Sagemaker Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5109f-902b-4980-983c-0cc55ee1be4c",
   "metadata": {},
   "source": [
    "* Author : Dipjyoti Das\n",
    "* Last Edited : Feb 7, 2024\n",
    "* This notebook provides an example for how to use Sagemaker Jumpstart -for text summarization use case on a web document. It uses Llama-13b-chat fine tuned open source model from Jumsptart model hub with Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368acb2d-fd10-4689-9a89-e379ef114da9",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "* AWS Innovation Sandbox should be installed and Domain created in Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f709c05-370c-4f18-ade3-74d099c29bd1",
   "metadata": {},
   "source": [
    "* Before deploying model endpoint on an instance, you have to request Service Quota limit increase for that particular instance. Check the form link on the confluence page. It might take upto a day for the request to be approved for the associated Sagemaker account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01b647-3c81-4199-8fe4-4386d2715bd3",
   "metadata": {},
   "source": [
    "* Upload the example pdf document ('6_extracted_FM-esg-report-2022.pdf') from Conflunce on your Sagemaker Jupyterlab folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed01b1f-d9cc-4417-bb93-e0611ceb8993",
   "metadata": {},
   "source": [
    "#### Minimum Instance sizes for the following Llama2 Foundational models in Jumpstart:\n",
    "* Llama2-7b-chat : ml.g5.2xlarge\n",
    "* Llama2-13b-chat : ml.g5.12xlarge\n",
    "* Llama2-70b-chat : ml.g5.48xlarge or ml.p4d.24xlarge\n",
    "* Pls request service quota increase for instance associated with the Sagemaker account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b793035-8e11-42fd-8773-c04fede5623e",
   "metadata": {},
   "source": [
    "#### Endpoint Names of the deployed Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a364788d-484f-42c5-a7b4-720192907fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Endpoint name and InferenceComponentName after you deploy the llama2_7b_chat model from Jumpstart model hub on an instance (ml.g5.2xlarge). \n",
    "# Navigate to Endpoint summary -> Test Inference -> Testing Options ->  Use Python SDK example code -> Example inference request\n",
    "\n",
    "llama2_13b_chat_endpoint_name = 'jumpstart-dft-meta-textgeneration-l-20240207-182557'\n",
    "llama2_13b_chat_InferenceComponentName = 'meta-textgeneration-llama-2-13b-f-20240207-182600'\n",
    "\n",
    "# Get the region name from the Sagemaker account\n",
    "region_name = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2241a07-4799-48f3-b029-11a7e6e58fef",
   "metadata": {},
   "source": [
    "#### Install and import the required Libraries with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ad2ca7-8254-4ba9-bb6f-3897a71cc89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.28.64\n"
     ]
    }
   ],
   "source": [
    "# Import the Boto3 and JSON modules\n",
    "import json\n",
    "import boto3\n",
    "print(boto3.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8437f315-a37e-42b8-97c2-831d5f1db87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers pypdfium2 accelerate langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27062064-1102-45f5-a3fd-6b36990655c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant modules and break down the long document into chunks:\n",
    "import langchain\n",
    "from langchain import SagemakerEndpoint, PromptTemplate\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain import LLMChain\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFium2Loader\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee9c96f8-b915-46d0-bfdb-c4a86e5f58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filepath of the Fannie Mae public docucment uploaded in the sagemaker directory:\n",
    "pdf_filepath = '/home/sagemaker-user/6_extracted_FM-esg-report-2022.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6b8b935-e092-4983-a5c7-48e8df83900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a pdf document and split in into chunks using Character Text Splitter function from Langchain\n",
    "def pdf_output(pdf_filepath):\n",
    "    loader = PyPDFium2Loader(pdf_filepath)\n",
    "    data = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts_FM1 = text_splitter.split_documents(data)\n",
    "    return texts_FM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f8c65a4-0bb2-40a0-9b4a-e1759f779764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='About Fannie Mae\\r\\nWho we are\\r\\nThe Federal National Mortgage Association, better known as \\r\\nFannie Mae, is a purpose-driven company by charter and by \\r\\nchoice. Our business supports mortgage lenders by providing \\r\\nmortgage financing to help people buy or rent a home. We help \\r\\nmake the popular 30-year fixed-rate mortgage possible, enabling \\r\\npredictable mortgage payments over the life of the loan and \\r\\ngiving homeowners stability and peace of mind. \\r\\nOur charter, an act of Congress, establishes our purposes: \\r\\nto provide liquidity and stability to the residential mortgage \\r\\nmarket and to promote access to mortgage credit. This mandate \\r\\nincludes facilitating mortgages on housing for low- and \\r\\nmoderate-income families involving a reasonable economic \\r\\nreturn that may be less than the return earned on other \\r\\nactivities. Congress declared that our operations should be \\r\\nfinanced by private capital to the maximum extent feasible. With \\r\\nthese Congressional intentions in mind, we have, principally \\r\\nusing private capital, provided liquidity in the secondary market \\r\\nand expanded housing opportunities throughout the U.S. Fannie \\r\\nMae is committed to maintaining safety and soundness as we \\r\\nwork to fulfill this mission.\\r\\nWe do not originate mortgage loans or lend money directly to \\r\\nborrowers. Rather, we work primarily with lenders who originate \\r\\nloans to borrowers. We acquire and securitize those loans into \\r\\nmortgage-backed securities that we guarantee (which we refer \\r\\nto as Fannie Mae MBS or our MBS). Our revenues are primarily \\r\\ndriven by guaranty fees we receive for assuming the credit risk on \\r\\nloans underlying our MBS. As of December 31, 2022, we owned \\r\\nor guaranteed mortgage assets representing an estimated \\r\\n27% of single-family mortgage debt outstanding and 21% of \\r\\nmultifamily mortgage debt outstanding in the U.S.7\\r\\n7 Based on internal estimates using data from the Federal Reserve Board of Governors’ “Financial Accounts of the United States” (Z.1) release, Q4 2022.\\r\\n© 2023 Fannie Mae 2022 ESG Report | 5\\r\\nTABLE OF CONTENTS INTRODUCTION / ABOUT FANNIE MAE', metadata={'source': '/home/sagemaker-user/6_extracted_FM-esg-report-2022.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the contents of the pdf document:\n",
    "pdf_output(pdf_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110afed-cd3b-4fc0-b06d-a983110eb779",
   "metadata": {},
   "source": [
    "#### Using Langchain for Text Summarization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa41be8-ff3d-4534-8aa6-970990b661ab",
   "metadata": {},
   "source": [
    "* Check :  https://python.langchain.com/docs/integrations/llms/sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc12e9f-79a7-45d3-b2f8-4c1e0860c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make LangChain work effectively with Llama models, we need to define the default Content Handler classes for valid input and output:\n",
    "\n",
    "class ContentHandlerTextSummarization(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> json:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        generated_text = response_json[0]['generated_text']\n",
    "        return generated_text.split(\"summary:\")[-1]\n",
    "    \n",
    "content_handler = ContentHandlerTextSummarization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ffd55-16b9-466f-a7d3-5acd47846afa",
   "metadata": {},
   "source": [
    "* Define the model with parameters using Sagemaker Endpoint class from Langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03a02624-bd44-42ce-996b-33d0bb282260",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model_llm = SagemakerEndpoint( endpoint_name=llama2_7b_chat_endpoint_name, \n",
    "                                      region_name= region_name,\n",
    "                                      model_kwargs={\"max_new_tokens\": 2000, \"top_p\": 0.9, \"temperature\": 0.6, \"top_k\":10, \"do_sample\" :True, \"max_length\": 1000},\n",
    "                                      endpoint_kwargs={ \"CustomAttributes\": 'accept_eula=true', \"InferenceComponentName\" : llama2_7b_chat_InferenceComponentName}, \n",
    "                                      content_handler=content_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0469e15e-f635-4299-813d-081b9f5f129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] template='\\n              Write a summary of the following text delimited by triple backquotes.\\n              Return your response in bullet points which covers the key points of the text.\\n              ```{text}```\\n              BULLET POINT SUMMARY:\\n           '\n"
     ]
    }
   ],
   "source": [
    "# Write the custom Prompt Template:\n",
    "template = \"\"\"\n",
    "              Write a summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "           \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91ce6435-2fd8-4443-8077-0b2e4691221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the LLMChain from Langchain library:\n",
    "llm_chain = LLMChain(prompt=prompt, llm=summary_model_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0884b5a-7359-4063-84e3-ff0cb971b2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • Fannie Mae is a purpose-driven company by charter and by choice,\n"
     ]
    }
   ],
   "source": [
    "# Result of Llama-2-7b-chat model\n",
    "print(llm_chain.run(summarize_pdf(pdf_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84872e-ab1c-45c7-9dfb-04ceefac4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summary may not be exactly what you are looking, you have to test it with Llama2-7b-chat model parameters or test with\n",
    "# bigger Llama2-13b or Llama2-70b parameter models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171bbada-1d1a-4b09-857f-6316ffa3e8a6",
   "metadata": {},
   "source": [
    "#### * Putting it all together in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "643ba51c-66b2-4841-9e8b-a90d7a15db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_filepath):\n",
    "    loader = PyPDFium2Loader(pdf_filepath)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    text_FM = text_splitter.split_documents(data)\n",
    "\n",
    "    template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "           \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=summary_model_llm)\n",
    "\n",
    "    return llm_chain.run(text_FM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6c3fd946-d003-4d19-b84d-815be0f2f755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' • Fannie Mae is a purpose-driven company by charter and by choice.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result of Llama2-7b-chat model\n",
    "summarize_pdf(pdf_filepath = pdf_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa463c3-5a48-42af-8542-e803dc3c6ac1",
   "metadata": {},
   "source": [
    "#### Using Gradio UI Interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59c3141d-ffe9-4be5-b7ed-5ae14cdaf686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "90d178c6-9b63-4455-826d-5ec7c1575f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.15.0\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "print(gr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a357d2a2-79b6-4f75-91e4-96a9d62e9ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/6_extracted_FM-esg-report-2022.pdf\n"
     ]
    }
   ],
   "source": [
    "# Get the filepath of the uploaded pdf document in Sagemkaer jupyterlab workspace:\n",
    "print(pdf_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539657e-82a8-448b-9b8a-bef42b677131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the above pdf_filepath in the Gradio: 'Provide PDF file path to get the summary' textbox field and you will see the summarized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa924f08-04d3-4bca-8330-c4401bc828b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "Running on public URL: https://fff323c07ca7ee6033.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://fff323c07ca7ee6033.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_pdf_path = gr.components.Textbox(label=\"Provide the PDF file path\")\n",
    "output_summary = gr.components.Textbox(label=\"Summary\")\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=summarize_pdf_full,\n",
    "    inputs=input_pdf_path,\n",
    "    outputs=output_summary,\n",
    "    title=\"PDF Summarizer\",\n",
    "    description=\"Provide PDF file path to get the summary.\"\n",
    ").queue().launch(share=True, debug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a5374-d1d5-45f8-992f-6c584658fb53",
   "metadata": {},
   "source": [
    "### Don't Forget : Delete the endpoints from the Llama2-7b-chat model notebook, also confirm deletion from the Jumpstart Deployment UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c1d238-2138-4758-a369-844418a12687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from configparser import ConfigParser\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# map reduce libraries:\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import MapReduceDocumentsChain\n",
    "from langchain.chains import ReduceDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2057acd6-6a0c-4020-a48c-37a5a3f00e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f71566-6dd9-490f-94be-13fd73b11367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting everything in a function:\n",
    "\n",
    "def summarize_web_map_reduce(map_template, reduce_template, url):\n",
    "    \n",
    "    # Create LLM\n",
    "    llm = ChatOpenAI(openai_api_key=openai.api_key)\n",
    "    # Map Chain\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "    map_chain = LLMChain(prompt=map_prompt, llm=llm)\n",
    "    # Reduce Chain\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    reduce_chain = LLMChain(prompt=reduce_prompt, llm=llm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Call the LLM Stuff Documents Chain\n",
    "    stuff_chain = StuffDocumentsChain(llm_chain=reduce_chain, document_variable_name=\"doc_summaries\")\n",
    "    # reduce chain:\n",
    "    reduce_chain = ReduceDocumentsChain(combine_documents_chain=stuff_chain,)\n",
    "    \n",
    "    # Map Reduce Chain\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        llm_chain=map_chain,\n",
    "        document_variable_name=\"content\",\n",
    "        reduce_documents_chain=reduce_chain)\n",
    "    \n",
    "    # load the content from web url through Langchain WebBaseLoader:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Split the content into smaller chuncks\n",
    "    splitter = TokenTextSplitter(chunk_size=2000)\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    \n",
    "    # Use Map reduce chain to summarize\n",
    "    summary = map_reduce_chain.run(split_docs)\n",
    " \n",
    "    return print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3c3d9d2-3e57-41c1-a62e-1cf3a68e293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'\n",
    "# Create the map template:\n",
    "map_template = \"\"\"Write a concise summary of the following content:\n",
    "\n",
    "{content}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Reduce Chain template\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "\n",
    "{doc_summaries}\n",
    "\n",
    "Write a concise summary of the the above summaries with all the key details\n",
    "provide the output in bullet points:\n",
    "Summary:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0e1222-e4c0-4ea7-8526-c80b8e3aa8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The article focuses on prompt engineering for autoregressive language models (LLMs) without updating model weights.\n",
      "- It discusses various techniques such as zero-shot and few-shot learning, instruction prompting, and augmented language models.\n",
      "- The importance of prompt engineering for model steerability and alignment is emphasized.\n",
      "- The content highlights effective instruction models and techniques, emphasizing specificity and avoiding negative instructions.\n",
      "- It explores methods like self-consistency sampling and chain-of-thought prompting to improve reasoning accuracy.\n",
      "- Automatic prompt design techniques like AutoPrompt and Prompt-Tuning are discussed.\n",
      "- Controllable neural text generation techniques like P-tuning, Prompt-Tuning, and APE are covered.\n",
      "- The use of iterative Monte Carlo search and chain-of-thought prompts for improving instruction candidates is explored.\n",
      "- The content discusses the augmentation of language models with reasoning skills and external tools.\n",
      "- It mentions the use of programming language statements and external APIs for resolving natural language reasoning problems.\n",
      "- The benefits of self-play iterations in improving model performance are highlighted.\n",
      "- Toolformer is introduced as a language model that can utilize external tools through simple APIs.\n",
      "- The model is trained in a self-supervised manner using demonstrations and fine-tuned on an annotated dataset.\n",
      "- The content includes a list of academic papers and preprints focused on various aspects of language models.\n"
     ]
    }
   ],
   "source": [
    "# Call the summarize web map reduce function:\n",
    "summarize_web_map_reduce(map_template=map_template, reduce_template=reduce_template, url=url1)\n",
    "# took around 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba366ce-02fe-4791-85c2-71b2d765d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_web_model_llm = SagemakerEndpoint(endpoint_name=llama2_13b_chat_endpoint_name, \n",
    "                                      region_name= region_name,\n",
    "                                      model_kwargs={\"max_new_tokens\": 2000, \"top_p\": 0.9, \"temperature\": 0.6, \"top_k\":10, \"do_sample\" :True, \"max_length\": 1000},\n",
    "                                      endpoint_kwargs={ \"CustomAttributes\": 'accept_eula=true', \"InferenceComponentName\" : llama2_13b_chat_InferenceComponentName}, \n",
    "                                      content_handler=content_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d3bf14f-f60e-4129-a015-f880c95aeffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SagemakerEndpoint(client=<botocore.client.SageMakerRuntime object at 0x7f65091e5000>, endpoint_name='jumpstart-dft-meta-textgeneration-l-20240207-182557', region_name='us-east-1', content_handler=<__main__.ContentHandlerTextSummarization object at 0x7f65582822c0>, model_kwargs={'max_new_tokens': 2000, 'top_p': 0.9, 'temperature': 0.6, 'top_k': 10, 'do_sample': True, 'max_length': 1000}, endpoint_kwargs={'CustomAttributes': 'accept_eula=true', 'InferenceComponentName': 'meta-textgeneration-llama-2-13b-f-20240207-182600'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_web_model_llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efe6cca1-3ec0-4329-a8ec-aec8228248df",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_web_model_llm2 = SagemakerEndpoint(endpoint_name=llama2_13b_chat_endpoint_name, \n",
    "                                      region_name= region_name,\n",
    "                                      model_kwargs={\"max_new_tokens\": 2000, \"top_p\": 0.9, \"temperature\": 0.6, \"top_k\":10, \"do_sample\" :True, \"max_length\": 1000},\n",
    "                                      endpoint_kwargs={ \"CustomAttributes\": 'accept_eula=true'}, \n",
    "                                      content_handler=content_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ec87368-c6ce-4bd3-9524-930509d6cefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SagemakerEndpoint(client=<botocore.client.SageMakerRuntime object at 0x7f6507256cb0>, endpoint_name='jumpstart-dft-meta-textgeneration-l-20240207-182557', region_name='us-east-1', content_handler=<__main__.ContentHandlerTextSummarization object at 0x7f65582822c0>, model_kwargs={'max_new_tokens': 2000, 'top_p': 0.9, 'temperature': 0.6, 'top_k': 10, 'do_sample': True, 'max_length': 1000}, endpoint_kwargs={'CustomAttributes': 'accept_eula=true'})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_web_model_llm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cf32da2-70e2-4e06-a97b-2a007694a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting everything in a function:\n",
    "\n",
    "def summarize_web_map_reduce2(map_template, reduce_template, url):\n",
    "    \n",
    "    # Create LLM\n",
    "    #llm = ChatOpenAI(openai_api_key=openai.api_key)\n",
    "    # Map Chain\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "    map_chain = LLMChain(prompt=map_prompt, llm=summary_web_model_llm)\n",
    "    # Reduce Chain\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    reduce_chain = LLMChain(prompt=reduce_prompt, llm=summary_web_model_llm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Call the LLM Stuff Documents Chain\n",
    "    stuff_chain = StuffDocumentsChain(llm_chain=reduce_chain, document_variable_name=\"doc_summaries\")\n",
    "    # reduce chain:\n",
    "    reduce_chain = ReduceDocumentsChain(combine_documents_chain=stuff_chain,)\n",
    "    \n",
    "    # Map Reduce Chain\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        llm_chain=map_chain,\n",
    "        document_variable_name=\"content\",\n",
    "        reduce_documents_chain=reduce_chain)\n",
    "    \n",
    "    # load the content from web url through Langchain WebBaseLoader:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Split the content into smaller chuncks\n",
    "    splitter = TokenTextSplitter(chunk_size=2000)\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    \n",
    "    # Use Map reduce chain to summarize\n",
    "    summary = map_reduce_chain.run(split_docs)\n",
    " \n",
    "    return print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fedf6ff-5d1b-4f5f-837f-ce16aa9c530a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['content'], template='Write a concise summary of the following content:\\n\\n{content}\\n\\nSummary:\\n')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad93e490-124a-4011-800c-7609309ddaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_map_chain = LLMChain(prompt=map_prompt, llm=summary_web_model_llm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a467cd2d-877f-4b4d-8be0-ca10499137a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['content'], template='Write a concise summary of the following content:\\n\\n{content}\\n\\nSummary:\\n'), llm=SagemakerEndpoint(client=<botocore.client.SageMakerRuntime object at 0x7f6507256cb0>, endpoint_name='jumpstart-dft-meta-textgeneration-l-20240207-182557', region_name='us-east-1', content_handler=<__main__.ContentHandlerTextSummarization object at 0x7f65582822c0>, model_kwargs={'max_new_tokens': 2000, 'top_p': 0.9, 'temperature': 0.6, 'top_k': 10, 'do_sample': True, 'max_length': 1000}, endpoint_kwargs={'CustomAttributes': 'accept_eula=true'}))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd try without inference component name\n",
    "llm_map_chain2 = LLMChain(prompt=map_prompt, llm=summary_web_model_llm2)\n",
    "llm_map_chain2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3357b275-a83b-4888-a0fb-ded1afcfc27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['doc_summaries'], template='The following is set of summaries:\\n\\n{doc_summaries}\\n\\nWrite a concise summary of the the above summaries with all the key details\\nprovide the output in bullet points:\\nSummary:')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce Chain\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f701c142-5ac0-4c4d-ac85-56d390499cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_reduce_chain = LLMChain(prompt=reduce_prompt, llm=summary_web_model_llm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2dc497b-0d74-4034-b0ba-f8991419ca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['doc_summaries'], template='The following is set of summaries:\\n\\n{doc_summaries}\\n\\nWrite a concise summary of the the above summaries with all the key details\\nprovide the output in bullet points:\\nSummary:'), llm=SagemakerEndpoint(client=<botocore.client.SageMakerRuntime object at 0x7f6507256cb0>, endpoint_name='jumpstart-dft-meta-textgeneration-l-20240207-182557', region_name='us-east-1', content_handler=<__main__.ContentHandlerTextSummarization object at 0x7f65582822c0>, model_kwargs={'max_new_tokens': 2000, 'top_p': 0.9, 'temperature': 0.6, 'top_k': 10, 'do_sample': True, 'max_length': 1000}, endpoint_kwargs={'CustomAttributes': 'accept_eula=true'})), document_variable_name='doc_summaries')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Call the LLM Stuff Documents Chain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_reduce_chain, document_variable_name=\"doc_summaries\")\n",
    "stuff_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f771eee-32f5-4ee2-8cd2-de9b1f6370bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['doc_summaries'], template='The following is set of summaries:\\n\\n{doc_summaries}\\n\\nWrite a concise summary of the the above summaries with all the key details\\nprovide the output in bullet points:\\nSummary:'), llm=SagemakerEndpoint(client=<botocore.client.SageMakerRuntime object at 0x7f6507256cb0>, endpoint_name='jumpstart-dft-meta-textgeneration-l-20240207-182557', region_name='us-east-1', content_handler=<__main__.ContentHandlerTextSummarization object at 0x7f65582822c0>, model_kwargs={'max_new_tokens': 2000, 'top_p': 0.9, 'temperature': 0.6, 'top_k': 10, 'do_sample': True, 'max_length': 1000}, endpoint_kwargs={'CustomAttributes': 'accept_eula=true'})), document_variable_name='doc_summaries'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # reduce chain:\n",
    "reduce_chain = ReduceDocumentsChain(combine_documents_chain=stuff_chain,)\n",
    "reduce_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "631927ff-7ac1-4215-9619-c46561edc250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['content'], template='Write a concise summary of the following content:\\n\\n{content}\\n\\nSummary:\\n'), llm=SagemakerEndpoint(client=<botocore.client.SageMakerRuntime object at 0x7f6507256cb0>, endpoint_name='jumpstart-dft-meta-textgeneration-l-20240207-182557', region_name='us-east-1', content_handler=<__main__.ContentHandlerTextSummarization object at 0x7f65582822c0>, model_kwargs={'max_new_tokens': 2000, 'top_p': 0.9, 'temperature': 0.6, 'top_k': 10, 'do_sample': True, 'max_length': 1000}, endpoint_kwargs={'CustomAttributes': 'accept_eula=true'})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['doc_summaries'], template='The following is set of summaries:\\n\\n{doc_summaries}\\n\\nWrite a concise summary of the the above summaries with all the key details\\nprovide the output in bullet points:\\nSummary:'), llm=SagemakerEndpoint(client=<botocore.client.SageMakerRuntime object at 0x7f6507256cb0>, endpoint_name='jumpstart-dft-meta-textgeneration-l-20240207-182557', region_name='us-east-1', content_handler=<__main__.ContentHandlerTextSummarization object at 0x7f65582822c0>, model_kwargs={'max_new_tokens': 2000, 'top_p': 0.9, 'temperature': 0.6, 'top_k': 10, 'do_sample': True, 'max_length': 1000}, endpoint_kwargs={'CustomAttributes': 'accept_eula=true'})), document_variable_name='doc_summaries')), document_variable_name='content')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  # Map Reduce Chain\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=llm_map_chain,\n",
    "    document_variable_name=\"content\",\n",
    "    reduce_documents_chain=reduce_chain)\n",
    "map_reduce_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb48a5bc-42f1-41dd-8dc7-0e7ef65a7c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "013827b7-88e5-46b1-9f65-7a0b2b68e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the content from web url through Langchain WebBaseLoader:\n",
    "loader = WebBaseLoader(url1)\n",
    "docs = loader.load()\n",
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0624f61-4b44-48f9-ac0a-5590f2e08270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the content into smaller chuncks\n",
    "splitter = TokenTextSplitter(chunk_size=2000)\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "944c05e1-fa22-4d5b-8a0c-db82d28624a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference endpoint: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component Name header is required for endpoints to which you plan to deploy inference components. Please include Inference Component Name header or consider using SageMaker models.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py:354\u001b[0m, in \u001b[0;36mSagemakerEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mAccept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccepts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_endpoint_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    979\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component Name header is required for endpoints to which you plan to deploy inference components. Please include Inference Component Name header or consider using SageMaker models.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use Map reduce chain to summarize\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mmap_reduce_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:503\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    504\u001b[0m         _output_key\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    509\u001b[0m         _output_key\n\u001b[1;32m    510\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:308\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    309\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    310\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    311\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:302\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    295\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    296\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    297\u001b[0m     inputs,\n\u001b[1;32m    298\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:119\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    118\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 119\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:222\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    212\u001b[0m     docs: List[Document],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    216\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     map_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[1;32m    228\u001b[0m     result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    229\u001b[0m         Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[1;32m    232\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:191\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    192\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n\u001b[1;32m    193\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:188\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    183\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    184\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    185\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_list},\n\u001b[1;32m    186\u001b[0m )\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:497\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    491\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    495\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    496\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:646\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    632\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    633\u001b[0m         )\n\u001b[1;32m    634\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    635\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    636\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m         )\n\u001b[1;32m    645\u001b[0m     ]\n\u001b[0;32m--> 646\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:534\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    533\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    535\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:521\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    513\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    518\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 521\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    525\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    529\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    530\u001b[0m         )\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:1043\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1042\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1047\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py:362\u001b[0m, in \u001b[0;36mSagemakerEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39minvoke_endpoint(\n\u001b[1;32m    355\u001b[0m         EndpointName\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name,\n\u001b[1;32m    356\u001b[0m         Body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_endpoint_kwargs,\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_handler\u001b[38;5;241m.\u001b[39mtransform_output(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# This is a bit hacky, but I can't figure out a better way to enforce\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# stop tokens when making calls to the sagemaker endpoint.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference endpoint: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component Name header is required for endpoints to which you plan to deploy inference components. Please include Inference Component Name header or consider using SageMaker models."
     ]
    }
   ],
   "source": [
    "# Use Map reduce chain to summarize\n",
    "summary = map_reduce_chain.run(split_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc4ad494-faf4-447d-956a-606525630a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference endpoint: Parameter validation failed:\nUnknown parameter in input: \"InferenceComponentName\", must be one of: EndpointName, Body, ContentType, Accept, CustomAttributes, TargetModel, TargetVariant, TargetContainerHostname, InferenceId, EnableExplanations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py:354\u001b[0m, in \u001b[0;36mSagemakerEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mAccept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccepts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_endpoint_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:936\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    933\u001b[0m endpoint_url, additional_headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_endpoint_ruleset(\n\u001b[1;32m    934\u001b[0m     operation_model, api_params, request_context\n\u001b[1;32m    935\u001b[0m )\n\u001b[0;32m--> 936\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_request_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m resolve_checksum_context(request_dict, operation_model, api_params)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1007\u001b[0m, in \u001b[0;36mBaseClient._convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, endpoint_url, context, headers, set_user_agent_header)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_to_request_dict\u001b[39m(\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1000\u001b[0m     api_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     set_user_agent_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1006\u001b[0m ):\n\u001b[0;32m-> 1007\u001b[0m     request_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_to_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39minject_host_prefix:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/validate.py:381\u001b[0m, in \u001b[0;36mParamValidationDecorator.serialize_to_request\u001b[0;34m(self, parameters, operation_model)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report\u001b[38;5;241m.\u001b[39mhas_errors():\n\u001b[0;32m--> 381\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParamValidationError(report\u001b[38;5;241m=\u001b[39mreport\u001b[38;5;241m.\u001b[39mgenerate_report())\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serializer\u001b[38;5;241m.\u001b[39mserialize_to_request(\n\u001b[1;32m    383\u001b[0m     parameters, operation_model\n\u001b[1;32m    384\u001b[0m )\n",
      "\u001b[0;31mParamValidationError\u001b[0m: Parameter validation failed:\nUnknown parameter in input: \"InferenceComponentName\", must be one of: EndpointName, Body, ContentType, Accept, CustomAttributes, TargetModel, TargetVariant, TargetContainerHostname, InferenceId, EnableExplanations",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call the summarize web map reduce function:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msummarize_web_map_reduce2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 36\u001b[0m, in \u001b[0;36msummarize_web_map_reduce2\u001b[0;34m(map_template, reduce_template, url)\u001b[0m\n\u001b[1;32m     33\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_documents(docs)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Use Map reduce chain to summarize\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mmap_reduce_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:503\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    504\u001b[0m         _output_key\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    509\u001b[0m         _output_key\n\u001b[1;32m    510\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:308\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    309\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    310\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    311\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:302\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    295\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    296\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    297\u001b[0m     inputs,\n\u001b[1;32m    298\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:119\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    118\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 119\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:222\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    212\u001b[0m     docs: List[Document],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    216\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     map_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[1;32m    228\u001b[0m     result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    229\u001b[0m         Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[1;32m    232\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:191\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    192\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n\u001b[1;32m    193\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:188\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    183\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    184\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    185\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_list},\n\u001b[1;32m    186\u001b[0m )\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:497\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    491\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    495\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    496\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:646\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    632\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    633\u001b[0m         )\n\u001b[1;32m    634\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    635\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    636\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m         )\n\u001b[1;32m    645\u001b[0m     ]\n\u001b[0;32m--> 646\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:534\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    533\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    535\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:521\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    513\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    518\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 521\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    525\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    529\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    530\u001b[0m         )\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:1043\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1042\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1047\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py:362\u001b[0m, in \u001b[0;36mSagemakerEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39minvoke_endpoint(\n\u001b[1;32m    355\u001b[0m         EndpointName\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name,\n\u001b[1;32m    356\u001b[0m         Body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_endpoint_kwargs,\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_handler\u001b[38;5;241m.\u001b[39mtransform_output(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# This is a bit hacky, but I can't figure out a better way to enforce\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# stop tokens when making calls to the sagemaker endpoint.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference endpoint: Parameter validation failed:\nUnknown parameter in input: \"InferenceComponentName\", must be one of: EndpointName, Body, ContentType, Accept, CustomAttributes, TargetModel, TargetVariant, TargetContainerHostname, InferenceId, EnableExplanations"
     ]
    }
   ],
   "source": [
    "# Call the summarize web map reduce function:\n",
    "summarize_web_map_reduce2(map_template=map_template, reduce_template=reduce_template, url=url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2896c4e-8d0d-47b6-a29c-97162601d6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
