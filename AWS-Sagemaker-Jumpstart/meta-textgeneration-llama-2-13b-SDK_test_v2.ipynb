{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Fine-tune Llama-2 models on SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251624f9-1eb6-4051-a774-0a4ba83cabf5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy pre-trained Llama 2 model as well as fine-tune it for your dataset in domain adaptation or instruction tuning format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c4fcd-d6c5-4381-8425-1d224c0ac197",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Set up\n",
    "\n",
    "---\n",
    "We begin by installing and upgrading necessary packages. Restart the kernel after executing the cell below for the first time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.207.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.207.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.33.9)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.2)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.25.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.1.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.20.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.5.2)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.0.7)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.9 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.33.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2023.11.17)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker) (0.58.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Downloading sagemaker-2.207.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.207.0\n",
      "    Uninstalling sagemaker-2.207.0:\n",
      "      Successfully uninstalled sagemaker-2.207.0\n",
      "Successfully installed sagemaker-2.207.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {},
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "---\n",
    "\n",
    "First we will deploy the Llama-2 model as a SageMaker endpoint. To train/deploy 13B and 70B models, please change model_id to \"meta-textgeneration-llama-2-7b\" and \"meta-textgeneration-llama-2-70b\" respectively.\n",
    "\n",
    " For successful deployment, you must manually change the `accept_eula` argument in the model's deploy method to `True`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21104c26",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgeneration-llama-2-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644e6af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version = \"3.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "983e5b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-13b-2024-02-09-00-34-14-600\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-13b-2024-02-09-00-34-36-474\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-13b-2024-02-09-00-34-36-474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "pretrained_predictor = pretrained_model.deploy(accept_eula=True, instance_type='ml.g5.24xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ef3008-e340-4f6d-9d1a-9abe8213e36b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.base_predictor.Predictor at 0x7f045bb2db40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cf3c462-3c8d-4468-9ae6-c43d33fb3096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.base_predictor.Predictor at 0x7f03f46f0550>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new\n",
    "pretrained_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c4ef-eb89-4da6-8e28-c800adbfc4b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "Next, we invoke the endpoint with some sample queries. Later, in this notebook, we will fine-tune this model with a custom dataset and carry out inference using the fine-tuned model. We will also show comparison between results obtained via the pre-trained and the fine-tuned models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9dd3ab8-7f43-4721-ae92-6b421c364693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[JumpStartSerializablePayload at 0x7f0410549620: {'content_type': 'application/json', 'accept': 'application/json', 'body': {'inputs': 'I believe the meaning of life is', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.6, 'decoder_input_details': True, 'details': True}}},\n",
       " JumpStartSerializablePayload at 0x7f04105491c0: {'content_type': 'application/json', 'accept': 'application/json', 'body': {'inputs': 'Simply put, the theory of relativity states that ', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.6}}},\n",
       " JumpStartSerializablePayload at 0x7f04105484a0: {'content_type': 'application/json', 'accept': 'application/json', 'body': {'inputs': 'A brief message congratulating the team on the launch:\\n\\nHi everyone,\\n\\nI just ', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.6}}},\n",
       " JumpStartSerializablePayload at 0x7f0410549e40: {'content_type': 'application/json', 'accept': 'application/json', 'body': {'inputs': 'Translate English to French:\\nsea otter => loutre de mer\\npeppermint => menthe poivrée\\nplush girafe => girafe peluche\\ncheese =>', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.6}}},\n",
       " JumpStartSerializablePayload at 0x7f04105499e0: {'content_type': 'application/json', 'accept': 'application/json', 'body': {'inputs': 'Please tell me a story.', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.2, 'decoder_input_details': True, 'details': True}}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_payloads = pretrained_model.retrieve_all_examples()\n",
    "example_payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd833f8-1ddc-4805-80b2-19e7db629880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input\n",
      " {'inputs': 'I believe the meaning of life is', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.6, 'decoder_input_details': True, 'details': True}} \n",
      "\n",
      "Output\n",
      "  to live life to the fullest.\n",
      "I believe in the power of love, the power of friendship, the power of family, the power of hard work, the power of goodness, the power of hope, the power of faith, and the power of prayer.\n",
      "I believe in the power of dreams \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': 'Simply put, the theory of relativity states that ', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.6}} \n",
      "\n",
      "Output\n",
      " 1) nothing can travel faster than the speed of light, and 2) the speed of light is constant. These two statements are not contradictory.\n",
      "The speed of light is constant because it is the maximum speed that anything can travel. This is because, as we’ve learned in earlier sections, mass and \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': 'A brief message congratulating the team on the launch:\\n\\nHi everyone,\\n\\nI just ', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.6}} \n",
      "\n",
      "Output\n",
      " wanted to  congratulate the team on the launch of this\n",
      "amazing product.\n",
      "\n",
      "It has been a pleasure working with you on this project.\n",
      "\n",
      "I look forward to seeing how this project will change the world.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Steve\n",
      " \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': 'Translate English to French:\\nsea otter => loutre de mer\\npeppermint => menthe poivrée\\nplush girafe => girafe peluche\\ncheese =>', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.6}} \n",
      "\n",
      "Output\n",
      "  fromage\n",
      "English to French translator :\n",
      "The English to French translator is free, online, and instant.\n",
      "It is also a good tool for learning French.\n",
      "It is also a good tool for learning English.\n",
      "How to use the English to French translator\n",
      "The English to French translator is \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': 'Please tell me a story.', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.2, 'decoder_input_details': True, 'details': True}} \n",
      "\n",
      "Output\n",
      " \n",
      "I’m not sure if you’ve noticed, but I’ve been a little quiet lately. I’ve been working on a few projects, and I’ve been busy with work, and I’ve been busy with life. But I’ve also been busy with my own thoughts.\n",
      " \n",
      "\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "for payload in example_payloads:\n",
    "    response = pretrained_predictor.predict(payload.body)\n",
    "    print(\"\\nInput\\n\", payload.body, \"\\n\\nOutput\\n\", response[0][\"generated_text\"], \"\\n\\n===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6773e6-7cf2-4cea-bce6-905d5995d857",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "To learn about additional use cases of pre-trained model, please checkout the notebook [Text completion: Run Llama 2 models in SageMaker JumpStart](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-text-completion.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Please find more details in the section [Dataset instruction](#Dataset-instruction). In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "\n",
    "Training data is formatted in JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The training folder can also contain a template.json file describing the input and output formats.\n",
    "\n",
    "To train your model on a collection of unstructured dataset (text files), please see the section [Example fine-tuning with Domain-Adaptation dataset format](#Example-fine-tuning-with-Domain-Adaptation-dataset-format) in the Appendix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda69bce-a940-41dc-b9f5-0941dd30f4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a1ff53-3fe4-4aa2-a6a9-4027a3e255d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category'],\n",
       "    num_rows: 15011\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dolly_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d01fc25d-c7e8-42a5-99e5-afcef636ec6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba5c250f-21e5-4746-9adf-4c23ed8db1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response'],\n",
       "    num_rows: 1188\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd14b3fd-6f8f-476e-9e81-533935d0006b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b90d75aa2de451c9a95f83676999ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2122360"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Where does the name Busan (city in Korea) come from?',\n",
       " 'context': 'The name \"Busan\" is the Revised Romanization of the city\\'s Korean name since the late 15th century. It officially replaced the earlier McCune-Reischauer romanization Pusan in 2000. During the Japanese period it was spelled \"Fuzan\".  The name 釜山 (now written 부산 using the Korean alphabet) is Sino-Korean for \"Cauldron Mountain\", believed to be a former name of Mt Hwangryeong (황령산, 荒嶺山, Hwangryeong-san) west of the city center. The area\\'s ancient state Mt Geochil (거칠산국, 居柒山國, Geochilsan-guk, \"Rough-Mountain Land\") is similarly thought to refer to the same mountain, which towers over the town\\'s harbor on the Suyeong. (The later Silla district of Geochilsan-gun was renamed Dongnae in 757.)',\n",
       " 'response': '\"Busan\" is the romanization of the city\\'s Korean name - 부산.  Previously, the name was romanized as \"Pusan\" until it was officially replaced in 2000.  The meaning of the name in Sino-Korean is \"Cauldron Mountain\", believed to be the former name of a nearby mountain now known as Mt Hwangryeong (황령산).'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job (since we are instruction fine-tuning the model in this example), and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-east-1-732939366832/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA v2 7B model on the summarization dataset from Dolly. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). To learn more about the fine-tuning scripts, please checkout section [5. Few notes about the fine-tuning method](#5.-Few-notes-about-the-fine-tuning-method). For a list of supported hyper-parameters and their default values, please see section [3. Supported Hyper-parameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for training job. Defaulting to ml.g5.24xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for training job. Defaulting to ml.g5.24xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True,  # For Llama-2-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"5\", max_input_length=\"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d3bea70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-13b-2024-02-08-22-47-42-159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-08 22:47:42 Starting - Starting the training job\n",
      "2024-02-08 22:47:42 Pending - Training job waiting for capacity......\n",
      "2024-02-08 22:48:15 Pending - Preparing the instances for training.......................................\n",
      "2024-02-08 22:54:53 Downloading - Downloading input data....................................\n",
      "2024-02-08 23:01:10 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-02-08 23:01:11,393 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-02-08 23:01:11,459 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-08 23:01:11,468 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-02-08 23:01:11,470 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-02-08 23:01:19,320 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nccl-cu12/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/triton/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.8.0-py3-none-any.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.4-py2.py3-none-any.whl (from -r requirements.txt (line 41))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 36)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 36)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 36)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 36)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 37)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.0->-r requirements.txt (line 36)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.0->-r requirements.txt (line 36)) (1.3.0)\u001b[0m\n",
      "\u001b[34mscipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=eda654b5c1e89fb5e995ce3bd9b364cb0c4a08ee1e376bb214afc0312be39d44\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, typing-extensions, triton, tokenize-rt, termcolor, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pyppmd, pycryptodomex, pybcj, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, multivolumefile, loralib, inflate64, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fire, black, transformers, nvidia-cusolver-cu12, torch, datasets, accelerate, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.4 sagemaker-jumpstart-script-utilities-1.1.9 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.1.0 transformers-4.31.0 triton-2.1.0 typing-extensions-4.8.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-02-08 23:02:11,532 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-02-08 23:02:11,532 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-02-08 23:02:11,621 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-08 23:02:11,696 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-08 23:02:11,772 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-08 23:02:11,782 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-13b-2024-02-08-22-47-42-159\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-13b-2024-02-08-22-47-42-159\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-02-08 23:02:11,812 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2024-02-08 23:02:17,439] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-02-08 23:02:17,439] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-02-08 23:02:17,439] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-02-08 23:02:17,439] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 14979.66it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1945.41it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1069 examples [00:00, 59338.96 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 15818.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 3449.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 3387.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1694.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1685.69 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1645.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1618.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1618.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1630.27 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1612.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1612.66 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.67s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  6.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  7.03s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 13015.86432 Million params\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  6.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  7.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  6.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  7.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  6.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  7.13s/it]\u001b[0m\n",
      "\u001b[34mtrainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.05032552357220002\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.05032552357220002\u001b[0m\n",
      "\u001b[34mtrainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.05032552357220002\u001b[0m\n",
      "\u001b[34mtrainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.05032552357220002\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 453\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 114\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.1+cuda12.1\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.4892932176589966\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:19<08:57, 19.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:18<08:25, 18.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:18<08:29, 18.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/28 [00:18<08:30, 18.91s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.4249576330184937\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:34<07:14, 16.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:33<07:02, 16.27s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:33<07:01, 16.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/28 [00:33<07:03, 16.29s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.1801412105560303\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:48<06:31, 15.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:47<06:25, 15.43s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:47<06:26, 15.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/28 [00:47<06:24, 15.40s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.4349418878555298\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [01:03<06:04, 15.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [01:02<06:01, 15.05s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [01:02<06:00, 15.02s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 4/28 [01:02<06:00, 15.04s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.3918265104293823\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [01:17<05:43, 14.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [01:16<05:41, 14.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [01:16<05:41, 14.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 5/28 [01:16<05:40, 14.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [01:31<05:23, 14.72s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.302757740020752\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [01:32<05:25, 14.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [01:31<05:23, 14.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 6/28 [01:31<05:23, 14.72s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.4856345653533936\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [01:46<05:07, 14.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [01:45<05:06, 14.62s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [01:45<05:07, 14.62s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 7/28 [01:45<05:07, 14.62s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.310509204864502\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [02:01<04:51, 14.59s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [01:59<04:51, 14.56s/it]#015Training Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [02:00<04:51, 14.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 8/28 [02:00<04:51, 14.56s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.2083579301834106\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [02:15<04:36, 14.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [02:14<04:36, 14.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [02:14<04:36, 14.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 9/28 [02:14<04:36, 14.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [02:28<04:21, 14.52s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3365046977996826\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [02:30<04:21, 14.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [02:29<04:21, 14.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 10/28 [02:29<04:21, 14.53s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.2521698474884033\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [02:44<04:06, 14.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [02:43<04:06, 14.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [02:43<04:06, 14.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 11/28 [02:43<04:06, 14.51s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.1963094472885132\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [02:58<03:52, 14.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [02:57<03:51, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [02:57<03:51, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 12/28 [02:57<03:51, 14.50s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.375091791152954\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [03:13<03:37, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [03:12<03:37, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [03:12<03:37, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 13/28 [03:12<03:37, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.0808556079864502\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [03:27<03:22, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [03:26<03:22, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [03:26<03:22, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 14/28 [03:26<03:22, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.3631247282028198\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [03:42<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [03:41<03:08, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [03:41<03:08, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▎    #033[0m| 15/28 [03:41<03:08, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.5031112432479858\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [03:56<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [03:55<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [03:55<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 16/28 [03:55<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.3593034744262695\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [04:11<02:39, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [04:10<02:39, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [04:10<02:39, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 17/28 [04:10<02:39, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.4654422998428345\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [04:25<02:24, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [04:24<02:24, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [04:24<02:24, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 18/28 [04:24<02:24, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.2102943658828735\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [04:40<02:10, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [04:39<02:10, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [04:39<02:10, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 19/28 [04:39<02:10, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.127960443496704\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [04:54<01:55, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [04:53<01:55, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [04:53<01:55, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 20/28 [04:53<01:55, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.0875850915908813\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [05:09<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [05:08<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [05:08<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 21/28 [05:08<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.3533309698104858\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [05:23<01:26, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [05:22<01:26, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [05:22<01:26, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 22/28 [05:22<01:26, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.3597532510757446\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [05:38<01:12, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [05:37<01:12, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [05:36<01:12, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 23/28 [05:37<01:12, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2799034118652344\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [05:52<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [05:51<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [05:51<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 24/28 [05:51<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.0407816171646118\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [06:07<00:43, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [06:05<00:43, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [06:06<00:43, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 25/28 [06:05<00:43, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.3768577575683594\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [06:21<00:28, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [06:20<00:28, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [06:20<00:28, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 26/28 [06:20<00:28, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.180960774421692\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [06:36<00:14, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [06:35<00:14, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [06:34<00:14, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 27/28 [06:34<00:14, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.3482104539871216\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [06:49<00:00, 14.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [06:50<00:00, 14.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [06:49<00:00, 14.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [06:50<00:00, 14.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [06:49<00:00, 14.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [06:49<00:00, 14.62s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [06:49<00:00, 14.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 28/28 [06:49<00:00, 14.63s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 13 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 15 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 13 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:08,  4.60s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:08,  4.60s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.61s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:08,  4.60s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.53s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.53s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.54s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:26<01:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:26<01:42,  4.48s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:26<01:42,  4.48s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:26<01:42,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.48s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.48s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:33,  4.47s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:33,  4.47s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:33,  4.47s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:33,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.47s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.47s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.47s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.47s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.47s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.47s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.47s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.47s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.47s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.47s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.47s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.47s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.47s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.47s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.47s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.47s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.47s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.47s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.47s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.47s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.47s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.47s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.47s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.47s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.47s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.47s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:42<00:26,  4.47s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:42<00:26,  4.47s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:42<00:26,  4.47s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:42<00:26,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.47s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.47s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:51<00:17,  4.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:51<00:17,  4.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:51<00:17,  4.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:51<00:17,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.47s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.47s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:00<00:08,  4.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:00<00:08,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:00<00:08,  4.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:00<00:08,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.47s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.47s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.47s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.2792, device='cuda:0') eval_epoch_loss=tensor(1.1876, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.187605619430542\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=3.6816, train_epoch_loss=1.3034, epcoh time 410.9065249119999s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.41s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2974817752838135\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:14, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:14, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:14, 14.42s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.2560198307037354\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:14, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.9961116909980774\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.2120873928070068\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.166884183883667\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:18, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1441426277160645\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:18, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:18, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:18, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.3333314657211304\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.47s/it]#015Training Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.2031733989715576\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:35, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:35, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.0587412118911743\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:35, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:35, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.1744767427444458\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [02:39<04:06, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [02:39<04:06, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [02:39<04:06, 14.49s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.1448588371276855\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 11/28 [02:39<04:06, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.49s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.0629311800003052\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [03:08<03:37, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [03:08<03:37, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [03:08<03:37, 14.49s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.2753671407699585\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▋     #033[0m| 13/28 [03:08<03:37, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.9492878317832947\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [03:37<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [03:37<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.3037853240966797\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [03:37<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▎    #033[0m| 15/28 [03:37<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.49s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.4103518724441528\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.49s/it]#015Training Epoch1:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [04:06<02:39, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [04:06<02:39, 14.49s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.25908362865448\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [04:06<02:39, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 17/28 [04:06<02:39, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3668510913848877\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [04:35<02:10, 14.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [04:35<02:10, 14.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [04:35<02:10, 14.50s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.11152184009552\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 19/28 [04:35<02:10, 14.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.49s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.0392429828643799\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [05:04<01:41, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.0264861583709717\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [05:04<01:41, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2622779607772827\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.310257911682129\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [05:47<00:57, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.224489688873291\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [05:47<00:57, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [05:47<00:57, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 24/28 [05:47<00:57, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.982499361038208\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.3349483013153076\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [06:16<00:28, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [06:16<00:28, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [06:16<00:28, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 26/28 [06:16<00:28, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.121396541595459\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.2918905019760132\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.48s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 13 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 15 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 13 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 114\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.63s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:03,  4.56s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:03,  4.56s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:03,  4.56s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:03,  4.56s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.53s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.53s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.51s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.51s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.50s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:48,  4.50s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:48,  4.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:48,  4.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:36<01:34,  4.48s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:36<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:36<01:34,  4.48s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:36<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.49s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.49s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.49s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.49s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.49s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.48s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.48s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.48s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.48s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.48s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.49s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.49s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.1661, device='cuda:0') eval_epoch_loss=tensor(1.1525, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 1.152491569519043\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=3.2787, train_epoch_loss=1.1874, epcoh time 405.79757339599996s\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:28, 14.40s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:28, 14.40s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2482415437698364\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:28, 14.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:28, 14.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:14, 14.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:14, 14.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:14, 14.42s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.2039661407470703\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:14, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:00, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:00, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:00, 14.42s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.9511198401451111\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:00, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.44s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.1657276153564453\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.124988317489624\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:17, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:17, 14.44s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1091551780700684\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:17, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:17, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.2915544509887695\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:48, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.1780531406402588\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [02:09<04:34, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [02:09<04:34, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.0201964378356934\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.130666971206665\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [02:38<04:05, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [02:38<04:05, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.1149715185165405\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [02:38<04:05, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  39%|#033[34m███▉      #033[0m| 11/28 [02:38<04:05, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.0290591716766357\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [03:07<03:36, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [03:07<03:36, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.2519193887710571\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [03:07<03:36, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▋     #033[0m| 13/28 [03:07<03:36, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.9164914488792419\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [03:36<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [03:36<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [03:36<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.2778804302215576\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▎    #033[0m| 15/28 [03:36<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.3864814043045044\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:39, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:39, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:39, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.2291373014450073\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:39, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3360224962234497\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.0688894987106323\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.0025602579116821\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.0004597902297974\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.228299856185913\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.2882440090179443\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [05:46<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [05:46<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [05:46<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2007321119308472\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 24/28 [05:46<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.9503543376922607\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [06:15<00:28, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [06:15<00:28, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [06:15<00:28, 14.44s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.3155041933059692\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 26/28 [06:15<00:28, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.45s/it]#015Training Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.0924204587936401\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.2674709558486938\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 13 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 15 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 13 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 228\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.63s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.63s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.55s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.55s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.55s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.55s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.51s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.51s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.48s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.48s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:34,  4.48s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:34,  4.48s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.47s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.47s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.47s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.47s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.47s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.47s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.47s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.47s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.47s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.47s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.47s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.47s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.47s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.47s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.47s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.47s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.47s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.47s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.47s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.47s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.47s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.47s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.47s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.47s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:00<00:08,  4.48s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:00<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.48s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.48s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.48s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:09<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.1305, device='cuda:0') eval_epoch_loss=tensor(1.1412, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 2 is 1.1412032842636108\u001b[0m\n",
      "\u001b[34mEpoch 3: train_perplexity=3.1716, train_epoch_loss=1.1542, epcoh time 405.2658932879999s\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.42s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2277872562408447\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.42s/it]#015Training Epoch3:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:15, 14.45s/it]#015Training Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:15, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.1792559623718262\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:15, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:15, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.9271180033683777\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.1408580541610718\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:46, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.46s/it]#015Training Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.0972148180007935\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:18, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:18, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:18, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.0850905179977417\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:18, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.2694116830825806\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.159633994102478\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 0.9939391613006592\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.1100552082061768\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [02:39<04:06, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [02:39<04:06, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [02:39<04:06, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.093434453010559\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  39%|#033[34m███▉      #033[0m| 11/28 [02:39<04:06, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.0124608278274536\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [03:08<03:37, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [03:08<03:37, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [03:08<03:37, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.2362987995147705\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▋     #033[0m| 13/28 [03:08<03:37, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.9019718766212463\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [03:37<03:08, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [03:37<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.2604328393936157\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [03:37<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▎    #033[0m| 15/28 [03:37<03:08, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.3722491264343262\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:39, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.2098530530929565\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:39, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:39, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:39, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3193227052688599\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.49s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.048224687576294\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.9869891405105591\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.9833704233169556\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2132489681243896\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.2735317945480347\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.1842724084854126\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [05:47<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [05:47<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [05:47<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 24/28 [05:47<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.9321935176849365\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.2977403402328491\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [06:16<00:28, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [06:16<00:28, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [06:16<00:28, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 26/28 [06:16<00:28, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.0680714845657349\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.25546395778656\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 28/28 [06:45<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 13 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 15 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 13 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 342\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.63s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.63s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.54s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.54s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.54s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]#015evaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.50s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.50s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.50s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:48,  4.50s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:48,  4.50s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:48,  4.50s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:48,  4.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:36<01:34,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:36<01:34,  4.49s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:36<01:34,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:36<01:34,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.49s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.49s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.48s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.48s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.48s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.48s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.48s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.48s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.49s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.49s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.49s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.48s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.48s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.48s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.48s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.48s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.48s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.1248, device='cuda:0') eval_epoch_loss=tensor(1.1394, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 3 is 1.1393675804138184\u001b[0m\n",
      "\u001b[34mEpoch 4: train_perplexity=3.1095, train_epoch_loss=1.1345, epcoh time 405.597346904s\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.42s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2117482423782349\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/28 [00:14<06:29, 14.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:15, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:15, 14.44s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.1600487232208252\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:15, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/28 [00:28<06:15, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.9064505696296692\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/28 [00:43<06:01, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:47, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.122445821762085\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:47, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:47, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 4/28 [00:57<05:47, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.074133038520813\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 5/28 [01:12<05:32, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:17, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.064789891242981\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:17, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:17, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██▏       #033[0m| 6/28 [01:26<05:17, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.45s/it]#015Training Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.2506015300750732\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 7/28 [01:41<05:03, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.1438236236572266\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 8/28 [01:55<04:49, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 0.9711697697639465\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 9/28 [02:10<04:34, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.0944764614105225\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▌      #033[0m| 10/28 [02:24<04:20, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [02:38<04:05, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [02:38<04:05, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [02:38<04:05, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.0734459161758423\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  39%|#033[34m███▉      #033[0m| 11/28 [02:38<04:05, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 0.9975677728652954\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 12/28 [02:53<03:51, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [03:07<03:36, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [03:07<03:36, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [03:07<03:36, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.2205954790115356\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▋     #033[0m| 13/28 [03:07<03:36, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.8895466327667236\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 14/28 [03:22<03:22, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [03:36<03:07, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [03:36<03:07, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [03:36<03:07, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.241517186164856\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▎    #033[0m| 15/28 [03:36<03:07, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.3569426536560059\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 16/28 [03:51<02:53, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:38, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:38, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:38, 14.44s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.1894587278366089\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  61%|#033[34m██████    #033[0m| 17/28 [04:05<02:38, 14.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.45s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3041950464248657\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▍   #033[0m| 18/28 [04:20<02:24, 14.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.0282902717590332\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 19/28 [04:34<02:10, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.9731877446174622\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 20/28 [04:49<01:55, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.9651389122009277\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 21/28 [05:03<01:41, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.1977672576904297\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▊  #033[0m| 22/28 [05:18<01:26, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.259354591369629\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 23/28 [05:32<01:12, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [05:46<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [05:46<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [05:46<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.1685266494750977\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 24/28 [05:46<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.47s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.9137964248657227\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 25/28 [06:01<00:43, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [06:15<00:28, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.2788861989974976\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [06:15<00:28, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [06:15<00:28, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 26/28 [06:15<00:28, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.48s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.0438768863677979\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 27/28 [06:30<00:14, 14.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.2420930862426758\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 28/28 [06:44<00:00, 14.46s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 13 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 15 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 13 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 456\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/29 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.63s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/29 [00:04<02:09,  4.64s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.54s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.54s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.54s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/29 [00:09<02:02,  4.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m█         #033[0m| 3/29 [00:13<01:57,  4.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/29 [00:18<01:52,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 5/29 [00:22<01:47,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 6/29 [00:27<01:43,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 7/29 [00:31<01:38,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:34,  4.48s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:34,  4.48s/it]#015evaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 8/29 [00:35<01:34,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]#015evaluating Epoch:  31%|#033[32m███       #033[0m| 9/29 [00:40<01:29,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 10/29 [00:44<01:25,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 11/29 [00:49<01:20,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.47s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.47s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 12/29 [00:53<01:16,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.47s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.47s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 13/29 [00:58<01:11,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 14/29 [01:02<01:07,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.48s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.48s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 15/29 [01:07<01:02,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 16/29 [01:11<00:58,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 17/29 [01:16<00:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.48s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.48s/it]#015evaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 18/29 [01:20<00:49,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 19/29 [01:25<00:44,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]#015evaluating Epoch:  69%|#033[32m██████▉   #033[0m| 20/29 [01:29<00:40,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]#015evaluating Epoch:  72%|#033[32m███████▏  #033[0m| 21/29 [01:34<00:35,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 22/29 [01:38<00:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 23/29 [01:43<00:26,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 24/29 [01:47<00:22,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 25/29 [01:52<00:17,  4.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]#015evaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m████████▉ #033[0m| 26/29 [01:56<00:13,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 27/29 [02:01<00:08,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.49s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.49s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 28/29 [02:05<00:04,  4.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 29/29 [02:10<00:00,  4.48s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.1286, device='cuda:0') eval_epoch_loss=tensor(1.1406, device='cuda:0')\u001b[0m\n",
      "\u001b[34mEpoch 5: train_perplexity=3.0539, train_epoch_loss=1.1164, epcoh time 405.40553177200036s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 3.259084463119507\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.179189920425415\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 3.165839195251465\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.1520071029663086\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 406.5945740544\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 5.506837187000019\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.16it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.08it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-02-08 23:53:24,347 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-02-08 23:53:24,347 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-02-08 23:53:24,348 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-02-08 23:53:30 Uploading - Uploading generated training model\n",
      "2024-02-08 23:54:16 Completed - Training job completed\n",
      "Training seconds: 3563\n",
      "Billable seconds: 3563\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3889d9-1567-41ad-9375-fb738db629fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Studio Kernel Dying issue:  If your studio kernel dies and you lose reference to the estimator object, please see section [6. Studio Kernel Dead/Creating JumpStart Model from the training Job](#6.-Studio-Kernel-Dead/Creating-JumpStart-Model-from-the-training-Job) on how to deploy endpoint using the training job name and the model id. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e58e4d-e581-4f8a-9899-8981316cce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took almost 70 min \n",
    "# billable : 3600 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45280ea0-089c-4a6a-9c20-3c73fe1306e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.jumpstart.estimator.JumpStartEstimator at 0x7f03fc661810>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-13b-2024-02-09-00-26-25-383\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-13b-2024-02-09-00-26-25-383\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-13b-2024-02-09-00-26-25-383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy(instance_type='ml.g5.12xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All arrays must be of the same length\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "\n",
    "    pretrained_response = pretrained_predictor.predict(payload)\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generated_text\"])\n",
    "\n",
    "    #finetuned_response = finetuned_predictor.predict(payload)\n",
    "    #responses_after_finetuning.append(finetuned_response[0][\"generated_text\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30e07148-fb8b-4764-8d7f-da7bed85cde6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from non-finetuned model</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease tell me who Bishop Patrick MacMullan was and when he died.\\n\\n### Input:\\nBishop Patrick MacMullan (17 March 1752 – 25 October 1824) was an Irish Roman Catholic Prelate and 20th Bishop of Down and Connor.\\n\\nHe was a native of mid Down and details of his early life in the latter half of the eighteenth century are sketchy. It is believed he was ordained to the priesthood in 1775.\\n\\nHe received episcopal consecration on 2 September 1793, and the following year succeeded his distant cousin Hugh as Bishop of Down and Connor.\\n\\nIn 1814 he made a report to Rome on the state of his diocese (served by around 35 parish priests and a few curates) which although vague gives some indication of the state of the diocese.\\n\\nHe died on 25 October 1824 in the house of his nephew in Loughinisland and is buried at Loughinisland Graveyard.\\n\\nA notice of his death, circulated in many Irish newspapers noted that \"the Catholic Clergy of that diocese [Down and Connor] have been under the scriptural jurisdiction of this amiable Prelate for 31 years, during which he has presided over them with the politeness of a Gentleman, the abilities of a Theologian, and the meekness of a humble and exemplary Christian.\"\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Patrick MacMullan was an Irish Roman Catholic Bishop. He passed away on October 25th, 1824.</td>\n",
       "      <td>\\nBishop Patrick MacMullan (17 March 1752 – 25 October 1824) was an Irish Roman Catholic Prelate and 20th Bishop of Down and Connor.\\n\\nHe was a native of mid Down and details of his early life in the latter half of the eighteenth century are sketchy. It is believed he was ordained to the priesthood in 1775.\\n\\nHe received episcop</td>\n",
       "      <td>Bishop Patrick MacMullan was an Irish Roman Catholic Prelate and 20th Bishop of Down and Connor. He was a native of mid Down and details of his early life in the latter half of the eighteenth century are sketchy. It is believed he was ordained to the priesthood in 1775. He received episcopal consecration on 2 September 1793, and the following year succeeded his distant cousin Hugh as Bishop of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow many bowl games have the University of Georgia football team won?\\n\\n### Input:\\nThe Georgia Bulldogs football program represents the University of Georgia in the sport of American football. The Bulldogs compete in the Football Bowl Subdivision (FBS) of the National Collegiate Athletic Association (NCAA) and the Eastern Division of the Southeastern Conference (SEC). They play their home games at historic Sanford Stadium on the university's Athens, Georgia, campus. Georgia claims four consensus national championships (1942, 1980, 2021, and 2022); while the AP and Coaches Polls have each voted the Bulldogs the national champion three times (1980, 2021, and 2022). Georgia has also been named the National Champion by at least one polling authority in four other seasons (1920, 1927, 1946 and 1968).\\n\\nThe Bulldogs' other accomplishments include 16 conference championships, of which 14 are SEC championships, second-most in conference history, and appearances in 61 bowl games, second-most all-time.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The University of Georgia football team is defined by greatness.   Known as the Georgia Bulldogs (Dawgs) and compete in the Division 1 Southeastern Conference (SEC).   They play in the historic Sanford Stadium in Athens, Georgia and have appeared in 61 bowl games, second-most all time.   In 2022 and 2023 the Georgia Bulldogs won 2 consecutive National Championships.</td>\n",
       "      <td>\\n```\\nThe Georgia Bulldogs football program represents the University of Georgia in the sport of American football. The Bulldogs compete in the Football Bowl Subdivision (FBS) of the National Collegiate Athletic Association (NCAA) and the Eastern Division of the Southeastern Conference (SEC). They play their home games at historic Sanford Stadium on the university's Athens, Georgia, campus. Georgia claims four consensus national championships (194</td>\n",
       "      <td>The Georgia Bulldogs football program represents the University of Georgia in the sport of American football. The Bulldogs compete in the Football Bowl Subdivision (FBS) of the National Collegiate Athletic Association (NCAA) and the Eastern Division of the Southeastern Conference (SEC). They play their home games at historic Sanford Stadium on the university's Athens, Georgia, campus. Georgia claims four consensus national championships (1942,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nFrom the given text, Provide some info regarding consistory\\n\\n### Input:\\nIn the Roman Catholic Church a consistory is a formal meeting of the College of Cardinals called by the pope. There are two kinds of consistories, extraordinary and ordinary. An \"extraordinary\" consistory is held to allow the pope to consult with the entire membership of the College of Cardinals. An \"ordinary\" consistory is ceremonial in nature and attended by cardinals resident in Rome. For example, the pope elevates new cardinals to the College at a consistory; Pope Francis has called consistories for ceremonies of canonization.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>1. A consistory is a formal gathering of the College of Cardinals in the Roman Catholic Church that is summoned by the pope.\\n2. Consistories come in two varieties: remarkable and ordinary.\\n3. The pope can consult with the full College of Cardinals by calling a \"extraordinary\" consistory.\\n4. Cardinals who live in Rome attend \"ordinary\" consistories, which are ceremonial in nature.\\n5. Pope Francis has convened consistories for canonization ceremonies, for instance, where he elevates new cardinals to the College of Cardinals.</td>\n",
       "      <td>\\n\\n\\n### Hint:\\n\\n\\n\\n### Solution:\\n\\n\\n\\n### Submission:\\n\\n\\n\\n### Testing:\\n\\n\\n\\n### Notes:\\n\\n\\n\\n### References:\\n\\n\\n\\n### Further Reading:\\n\\n\\n\\n### Tags:\\n\\n\\n</td>\n",
       "      <td>A consistory is a formal meeting of the College of Cardinals called by the pope. There are two kinds of consistories, extraordinary and ordinary. An \"extraordinary\" consistory is held to allow the pope to consult with the entire membership of the College of Cardinals. An \"ordinary\" consistory is ceremonial in nature and attended by cardinals resident in Rome. For example, the pope elevates new cardinals to the College at a consistory; Pope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nList down some information about Ahilya Bai from given passage\\n\\n### Input:\\nAfter the demise of her husband Khande Rao Holkar and father-in-law Malhar Rao Holkar, Ahilya Bai herself undertook the affairs of Holkar dynasty. She defended the Malwa state against intruders and personally led armies into battle, with Tukoji Rao Holkar as her military commander.\\n\\nAhilya Bai was a great pioneer and builder of Hindu temples who constructed hundreds of temples and Dharmashalas throughout India. She is specially renowned for refurbishing &amp; reconsecrating some of the most sacred sites of Hindu pilgrimage that had been desecrated &amp; demolished in the previous century by the Mughal Emperor Aurangzeb\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>1.Ahilya bai Holkar under took the affairs of Holkar Dynasty after demise of her husband khande Rao Holkar and father in law Malhar Rao Holkar.\\n2.Ahilya Bai defended the Malwa state against intruders and personally led armies into battle, with Tukoji Rao Holkar as her milatary commander. \\n3.Ahilya Bai was a great pioneer and builder of Hindu temples and Dharmshalas through out India.\\n4.Ahilya Bai is specially renowned for refurbishing &amp; reconsecrating some of the most sacred sites of Hindu pilgrimage that had been demolished by the Mughal Emperor Aurangzeb.</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>1. Ahilya Bai was a great pioneer and builder of Hindu temples who constructed hundreds of temples and Dharmashalas throughout India.\\n2. She is specially renowned for refurbishing &amp; reconsecrating some of the most sacred sites of Hindu pilgrimage that had been desecrated &amp; demolished in the previous century by the Mughal Emperor Aurangzeb.\\n3. After the demise of her husband Kh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nUsing the text provided, give me the type of engineers that are involved in fabrication of a solar power plant\\n\\n### Input:\\nSolar power plants derive their energy from sunlight, which is made accessible via photovoltaics (PV's). Photovoltaic panels, or solar panels, are constructed using photovoltaic cells which are made of silica materials that release electrons when they are warmed by the thermal energy of the sun. The new flow of electrons generates electricity within the cell. While PV's are an efficient method of producing electricity, they do burn out after a decade and thus, must be replaced; however, their efficiency, cost of operation, and lack of noise/physical pollutants make them one of the cleanest and least expensive forms of energy. Solar power plants require the work of many facets of engineering; electrical engineers are especially crucial in constructing the solar panels and connecting them into a grid, and computer engineers code the cells themselves so that electricity can be effectively and efficiently produced, and civil engineers play the very important role of identifying areas where solar plants are able to collect the most energy.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Although solar power plants require the work of many aspects of engineering, the three main types of engineers involved in fabrication of solar power plants are:\\n1. Electrical engineers who build solar panels and link them to the electrical grid\\n2. Computer engineers who program the cells themselves so that power can be produced effectively and efficiently\\n3. Civil engineer who play a critical role in determining sites where the solar plants can capture the most energy.</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>The type of engineers that are involved in fabrication of a solar power plant are electrical engineers, computer engineers and civil engineers.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "\n",
    "    pretrained_response = pretrained_predictor.predict(payload)\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generated_text\"])\n",
    "\n",
    "    finetuned_response = finetuned_predictor.predict(payload)\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generated_text\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be179b53-d462-4da9-a7e0-27051b035a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-2-13b-2024-02-09-00-34-14-600\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-2-13b-2024-02-09-00-34-36-474\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-2-13b-2024-02-09-00-34-36-474\n"
     ]
    }
   ],
   "source": [
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {},
   "source": [
    "### 1. Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports many parameters while performing inference. They include:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches `max_new_tokens`. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must be a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments.\n",
    "\n",
    "**NOTE**: If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5df7e-95a5-47dc-b5d2-0178ebfc6b6f",
   "metadata": {},
   "source": [
    "### 2. Dataset formatting instruction for training\n",
    "\n",
    "---\n",
    "\n",
    "####  Fine-tune the Model on a New Dataset\n",
    "We currently offer two types of fine-tuning: instruction fine-tuning and domain adaption fine-tuning. You can easily switch to one of the training \n",
    "methods by specifying parameter `instruction_tuned` being 'True' or 'False'.\n",
    "\n",
    "\n",
    "#### 2.1. Domain adaptation fine-tuning\n",
    "The Text Generation model can also be fine-tuned on any domain specific dataset. After being fine-tuned on the domain specific dataset, the model\n",
    "is expected to generate domain specific text and solve various NLP tasks in that specific domain with **few shot prompting**.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file. \n",
    "  - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "  - The number of files under train and validation (if provided) should equal to one, respectively. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "```Note About Forward-Looking Statements\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "GENERAL\n",
    "Embracing Our Future ...\n",
    "```\n",
    "\n",
    "\n",
    "#### 2.2. Instruction fine-tuning\n",
    "The Text generation model can be instruction-tuned on any text data provided that the data \n",
    "is in the expected format. The instruction-tuned model can be further deployed for inference. \n",
    "Below are the instructions for how the training data should be formatted for input to the \n",
    "model.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Train and validation directories should contain one or multiple JSON lines (`.jsonl`) formatted files. In particular, train directory can also contain an optional `*.json` file describing the input and output formats. \n",
    "  - The best model is selected according to the validation loss, calculated at the end of each epoch.\n",
    "  If a validation set is not given, an (adjustable) percentage of the training data is\n",
    "  automatically split and used for validation.\n",
    "  - The training data must be formatted in a JSON lines (`.jsonl`) format, where each line is a dictionary\n",
    "representing a single data sample. All training data must be in a single folder, however\n",
    "it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training\n",
    "folder can also contain a `template.json` file describing the input and output formats. If no\n",
    "template file is given, the following template will be used:\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\",\n",
    "    \"completion\": \"{response}\"\n",
    "  }\n",
    "  ```\n",
    "  - In this case, the data in the JSON lines entries must include `instruction`, `context` and `response` fields. If a custom template is provided it must also use `prompt` and `completion` keys to define\n",
    "  the input and output templates.\n",
    "  Below is a sample custom template:\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"question: {question} context: {context}\",\n",
    "    \"completion\": \"{answer}\"\n",
    "  }\n",
    "  ```\n",
    "Here, the data in the JSON lines entries must include `question`, `context` and `answer` fields. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edf0c9-3c95-4e4b-932e-5d8a839d4070",
   "metadata": {},
   "source": [
    "#### 2.3. Example fine-tuning with Domain-Adaptation dataset format\n",
    "---\n",
    "We provide a subset of SEC filings data of Amazon in domain adaptation dataset format. It is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data).\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Please uncomment the following code to fine-tune the model on dataset in domain adaptation format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c93a9-ebe2-4966-a5d6-af4c053f69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# model_id = \"meta-textgeneration-llama-2-7b\"\n",
    "\n",
    "# estimator = JumpStartEstimator(model_id=model_id,  environment={\"accept_eula\": \"true\"},instance_type = \"ml.g5.24xlarge\")\n",
    "# estimator.set_hyperparameters(instruction_tuned=\"False\", epoch=\"5\")\n",
    "# estimator.fit({\"training\": f\"s3://jumpstart-cache-prod-{boto3.Session().region_name}/training-datasets/sec_amazon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### 3. Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6d023-3487-4571-8b52-f332790c1ad7",
   "metadata": {},
   "source": [
    "### 4. Supported Instance types\n",
    "\n",
    "---\n",
    "We have tested our scripts on the following instances types:\n",
    "\n",
    "- 7B: ml.g5.12xlarge, nl.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 13B: ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 70B: ml.g5.48xlarge\n",
    "\n",
    "Other instance types may also work to fine-tune. Note: When using p3 instances, training will be done with 32 bit precision as bfloat16 is not supported on these instances. Thus, training job would consume double the amount of CUDA memory when training on p3 instances compared to g5 instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d4350-cf4d-40a0-be1c-eba44efd33ab",
   "metadata": {},
   "source": [
    "### 5. Few notes about the fine-tuning method\n",
    "\n",
    "---\n",
    "- Fine-tuning scripts are based on [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). \n",
    "- Instruction tuning dataset is first converted into domain adaptation dataset format before fine-tuning. \n",
    "- Fine-tuning scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method fine-tuning the models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ce841-3a2c-4c08-a102-b94148036a5a",
   "metadata": {},
   "source": [
    "### 6. Studio Kernel Dead/Creating JumpStart Model from the training Job\n",
    "---\n",
    "Due to the size of the Llama 70B model, training job may take several hours and the studio kernel may die during the training phase. However, during this time, training is still running in SageMaker. If this happens, you can still deploy the endpoint using the training job name with the following code:\n",
    "\n",
    "How to find the training job name? Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa60a66-1c2f-42df-8079-191319e28a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# attached_estimator.deploy()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
