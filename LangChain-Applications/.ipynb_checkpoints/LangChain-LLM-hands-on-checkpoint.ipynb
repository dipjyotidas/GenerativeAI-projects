{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77f6189-b992-45e2-a0d7-50a639ca2a39",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067efe7-3073-4c41-92ea-d9b4de2a9bc5",
   "metadata": {},
   "source": [
    "LangChain is an open-source framework designed to facilitate the development of applications powered by large language models (LLMs). It offers a suite of tools, components, and interfaces that simplify the construction of LLM-centric applications. With LangChain, it becomes effortless to manage interactions with language models, seamlessly link different components, and incorporate resources such as APIs and databases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2cbd2b-47ba-462b-b307-402387bb1ae9",
   "metadata": {},
   "source": [
    "Applications like chatbots, virtual assistants, language translation utilities, and sentiment analysis tools are all instances of LLM-powered apps. Developers leverage LangChain to create bespoke language model-based applications that cater to specific needs.\n",
    "1. Tailorable prompts to meet your specific requirements\n",
    "2. Constructing chain link components for advanced usage scenarios\n",
    "3. Integrating models for data augmentation and accessing top-notch language model capabilities, such as GPT and HuggingFace Hub.\n",
    "4. Versatile components that allow mixing and matching for specific needs\n",
    "5. Manipulating context to establish and guide context for enhanced precision and user satisfaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda73d06-f2de-46fb-8192-3b5c610e9a4d",
   "metadata": {},
   "source": [
    "## Key Components of LangChain\n",
    "* LangChain stands out due to its emphasis on flexibility and modularity. It disassembles the natural language processing pipeline into separate components, enabling developers to tailor workflows according to their needs. This adaptability makes LangChain ideal for constructing AI applications across various scenarios and sectors.\n",
    "\n",
    "## Components and chains\n",
    "* In LangChain, components are modules performing specific functions in the language processing pipeline. These components can be linked into \"chains\" for tailored workflows, such as a customer service chatbot chain with sentiment analysis, intent recognition, and response generation modules.\n",
    "\n",
    "## Prompt templates\n",
    "* Prompt templates are reusable predefined prompts across chains. These templates can become dynamic and adaptable by inserting specific \"values.\" For example, a prompt asking for a user's name could be personalized by inserting a specific value. This feature is beneficial for generating prompts based on dynamic resources.\n",
    "\n",
    "## Vector stores\n",
    "* These are used to store and search information via embeddings, essentially analyzing numerical representations of document meanings. VectorStore serves as a storage facility for these embeddings, allowing efficient search based on semantic similarity.\n",
    "\n",
    "## Indexes and retrievers\n",
    "* Indexes act as databases storing details and metadata about the model's training data, while retrievers swiftly search this index for specific information. This improves the model's responses by providing context and related information.\n",
    "\n",
    "## Output parsers\n",
    "* Output parsers come into play to manage and refine the responses generated by the model. They can eliminate undesired content, tailor the output format, or supplement extra data to the response. Thus, output parsers help extract structured results, like JSON objects, from the language model's responses.\n",
    "\n",
    "## Example selectors\n",
    "* Example selectors in LangChain serve to identify appropriate instances from the model's training data, thus improving the precision and pertinence of the generated responses. These selectors can be adjusted to favor certain types of examples or filter out unrelated ones, providing a tailored AI response based on user input.\n",
    "\n",
    "## Agents\n",
    "* Agents are unique LangChain instances, each with specific prompts, memory, and chain for a particular use case. They can be deployed on various platforms, including web, mobile, and chatbots, catering to a wide audience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486c194-5835-427a-9dbb-8bc2aed80339",
   "metadata": {},
   "source": [
    "## Build A Language Model Application in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31a1545-84db-4d70-85c9-1bc05bc158bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "What data scientist did the data scientist?\n",
      "\n",
      "The data scientist found a data set that consisted of nothing but errors.\n",
      "\n",
      "How did the data scientist improve the data set?\n",
      "\n",
      "The data scientist improved the data set by adding more things to the data set.\n"
     ]
    }
   ],
   "source": [
    "# Set your API key : https://platform.openai.com/account/api-keys\n",
    "# store the API key in Environment variable : https://networkdirection.net/python/resources/env-variable/\n",
    "import os\n",
    "API_KEY = os.environ.get('OpenAI_API_Key')\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=API_KEY)\n",
    "# text-ada-001 model from OpenAI\n",
    "print(llm(\"Tell me a joke about data scientist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35bdbf11-d01e-42bb-a214-a6fa1885fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Access token kry from Hugging Face\n",
    "#API_KEY = os.environ.get('huggingface_hub_access_token')\n",
    "# from langchain import HuggingFaceHub\n",
    "# llm = HuggingFaceHub(repo_id = \"google/flan-t5-xl\", huggingfacehub_api_token = API_KEY)\n",
    "# open-source models from HuggingFace\n",
    "#print(llm(\"Tell me a joke about data scientist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc42530b-9dfb-44ab-ba1e-aa1bb5e96fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = llm.generate(['Tell me a joke about data scientist',\n",
    "'Tell me a joke about recruiter',\n",
    "'Tell me a joke about psychologist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1920d2-c1e9-4b54-aad9-3633e5c32a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.output.LLMResult"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1d5000a-7d3c-41b4-8f30-3d7b1b77b154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text='\\n\\nWhat do you call a data scientist who also happens to be a code writer? A \"code writer of record.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response.generations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "844147ae-18bc-4108-a2c0-c1b030a4276c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text='\\n\\nWhy did the recruiter find the other person?\\n\\nBecause he was the only one who could find him a job.', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response.generations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2cc19b2-11ef-4a23-a82f-fb00f34a1407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text='\\n\\nWhy did the psychologist stop working?\\n\\nBecause he was tired!', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response.generations[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddc479c-7696-42ed-af4d-403f4aa52f9d",
   "metadata": {},
   "source": [
    "### Managing Prompt Templates for LLMs in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b5aed-9a59-4f8f-b746-72c8a908488d",
   "metadata": {},
   "source": [
    "A PromptTemplate in LangChain allows you to use templating to generate a prompt. This is useful when you want to use the same prompt outline in multiple places but with certain values changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06ae89f7-af97-43b9-884f-cf44c045a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: \n",
      "\n",
      "1. Visit the iconic Eiffel Tower and take a stroll along the Champs-Élysées. \n",
      "2. Enjoy a day of shopping in the Marais district and explore the city's diverse art galleries. \n",
      "3. Take a boat ride along the Seine River and experience the city's breathtaking skyline.\n"
     ]
    }
   ],
   "source": [
    "USER_INPUT = 'Paris'\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=API_KEY)\n",
    "template = \"\"\" I am travelling to {location}. What are the top 3 things I can do while I am there. Be very specific and respond as three bullet points \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"location\"],\n",
    "template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=USER_INPUT )\n",
    "print(f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cbdc02e-9a9e-4797-9619-8f4c127d61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: \n",
      "\n",
      "- Visit the ancient Mayan ruins of Chichen Itza, one of the Seven Wonders of the World. \n",
      "- Relax on the stunning white sand beaches of the Riviera Maya. \n",
      "- Experience the vibrant nightlife of Playa del Carmen's famous 5th Avenue.\n"
     ]
    }
   ],
   "source": [
    "USER_INPUT = 'Cancun'\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=API_KEY)\n",
    "template = \"\"\" I am travelling to {location}. What are the top 3 things I can do while I am there. Be very specific and respond as three bullet points \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"location\"],\n",
    "template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=USER_INPUT )\n",
    "print(f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17a0d5-b142-4cd6-a2d4-6cbc47123989",
   "metadata": {},
   "source": [
    "### Combining LLMs and Prompts in Multi-Step Workflows\n",
    "\n",
    "* Chaining within the LangChain context refers to the act of integrating LLMs with other elements to build an application.\n",
    "\n",
    "* Sequentially combining multiple LLMs by using the output of the first LLM as input for the second LLM\n",
    "* Integrating LLMs with prompt templates\n",
    "* Merging LLMs with external data, such as for question answering\n",
    "* Incorporating LLMs with long-term memory, like chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dfce38-16a6-425b-a055-0825395d0a21",
   "metadata": {},
   "source": [
    "### Use the output from the first LLM as an input to the second LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6057b38c-9ef5-4bd3-a674-d4c1388e2962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Toronto.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "1. Explore the CN Tower and experience the 360-degree view of Toronto\n",
      "2. Visit the iconic waterfront for outdoor activities and amazing views\n",
      "3. Take a stroll through the historic Distillery District and explore the vibrant culture and nightlife\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=API_KEY)\n",
    "# first step in chain\n",
    "template = \"What is the most popular city in {country} for tourists? Just return the name of the city\"\n",
    "\n",
    "first_prompt = PromptTemplate(\n",
    "input_variables=[\"country\"],\n",
    "template=template)\n",
    "\n",
    "chain_one = LLMChain(llm = llm, prompt = first_prompt)\n",
    "\n",
    "# second step in chain\n",
    "second_prompt = PromptTemplate(\n",
    "\n",
    "input_variables=[\"city\"],\n",
    "template=\"What are the top three things to do in this: {city} for tourists. Just return the answer as three bullet points.\",)\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "# Combine the first and the second chain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "final_answer = overall_chain.run(\"Canada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d4405-19f0-486a-ba66-767becd805a6",
   "metadata": {},
   "source": [
    "In this particular example, we create a chain with two components. The first component is responsible for identifying the most popular city corresponding to a particular country as input by the user. In contrast, the second component focuses on providing information about the top three activities or attractions available for tourists visiting that specific city."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b04cac-e5a7-4b37-9ce1-2c1592746312",
   "metadata": {},
   "source": [
    "LangChain, an open-source Python framework, enables individuals to create applications powered by LLMs (Language Model Models). This framework offers a versatile interface to numerous foundational models, facilitating prompt management and serving as a central hub for other components such as prompt templates, additional LLMs, external data, and other tools through agents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4928a-668a-488f-877f-4a1737096240",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
